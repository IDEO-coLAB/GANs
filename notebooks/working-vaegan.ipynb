{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tried\n",
    "\n",
    "1. Adam, lr=.0003, gamma=1e-2\n",
    "- Adam, lr=0.0001, gamma=1e-2\n",
    "- Adam, lr=0.0001, beta1=0.5, gamma=1e-2\n",
    "- Adam, lr=0.0001, beta1=0.5, decoder + noise, mean=0, stddev=1.0\n",
    "- Adam, lr=0.0001, beta1=0.5, decoder loss = similarity only. \n",
    "    - This gets faces early, but quality of faces plateaus as discriminator gets too good. (Might want to try larger training set size than 512).\n",
    "- Adam, lr=0.0001, beta1=0.5, decoder loss = similarity only.\n",
    "- Adam, lr=0.0001, beta1=0.5, decoder loss = similarity + 0.1 * disc\n",
    "    - Both vae and x discriminator out go to 0, decoder wins\n",
    "- Adam, lr=0.0001, beta1=0.5, decoder loss = similarity + 0.01 * disc\n",
    "    - Both vae and x discriminator out go to 0, decoder wins\n",
    "- Adam, lr=0.0001, beta1=0.5, decoder loss = similarity only, training set = 5000\n",
    "    - Still looks like the discriminator is going to win but the rate at which the discriminator loss drops is slower than in 5 and image output is better.\n",
    "\n",
    "#### Adam, lr=0.0001, beta1=0.5, training set = 5000\n",
    "1. decoder loss = similarity only, vae and x noise sigma 10.0\n",
    "- moved similarity to after 1st conv in discriminator (prev. after 3rd), vae and x noise sigma 1.0, training 2x discr for each generator. \n",
    "    - Artifacts of first layer conv and weird coloring (blue and purple). Not sure why. Is there something wrong with the architecture?\n",
    "- similarity to after 4th conv in discriminator (prev. after 3rd), vae and x noise sigma 1.0, training 2x discr for each generator. \n",
    "\n",
    "#### Adam, lr=0.0001, beta1=0.5, training set = 5000, vae and x noise sigma 1.0, training 1x for enc, dec, disc\n",
    "- decoder loss = similarity + -log(disc(x)), label noise to disc\n",
    "    - Think I saw mode collapes\n",
    "- above but change label smoothing so disc 0 labels are 0, disc 1 labels are 0.75\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# discriminator loss thresholds:\n",
    "# below 0.3 stop training disc\n",
    "# above 0.5 train disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "\n",
    "# img_directory = '/Users/rwilliams/Desktop/celeba/training'\n",
    "img_directory = '/home/ec2-user/training-data/img_align_celeba'\n",
    "model_save_path = '/home/ec2-user/tf-checkpoints/vaegan-celeba/checkpoint.ckpt'\n",
    "outputs_directory = '/home/ec2-user/outputs/vaegan-celeba'\n",
    "log_directory = '/home/ec2-user/tf-logs/vaegan-celeba'\n",
    "\n",
    "batch_size = 64\n",
    "training_set_size = 1024\n",
    "img_size = 64\n",
    "learning_rate = 0.001\n",
    "beta1 = 0.9\n",
    "\n",
    "zsize = 128\n",
    "\n",
    "# weights similarity loss term for decoder loss\n",
    "loss_gamma = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import os\n",
    "from utils import imshow, resize_crop, load_img, pixels01, pixels11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Jupyter imports\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "\n",
    "# cache results of resizing and cropping on disk\n",
    "from joblib import Memory\n",
    "cachedir = '/home/ec2-user/joblib-cache'\n",
    "memory = Memory(cachedir=cachedir, verbose=0)\n",
    "\n",
    "@memory.cache\n",
    "def load_all_imgs(howmany, img_directory):\n",
    "    training = np.array([resize_crop(load_img(i+1, img_directory), (img_size, img_size)) for i in range(howmany)])\n",
    "    # rescale each pixel to [-1, 1]. Supposed to help with GANs\n",
    "    training = pixels11(training)\n",
    "    return training\n",
    "\n",
    "training = load_all_imgs(training_set_size, img_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create models\n",
    "\n",
    "import tensorflow as tf\n",
    "from autoencoder import Autoencoder\n",
    "from discriminator import Discriminator\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(42.0)\n",
    "\n",
    "# input images feed\n",
    "X = tf.placeholder(tf.float32, [None, img_size, img_size, 3])\n",
    "R = tf.placeholder(tf.float32, [None, img_size, img_size, 3])\n",
    "\n",
    "\n",
    "# for feeding random draws of z (latent variable)\n",
    "Z = tf.placeholder(tf.float32, [None, zsize])\n",
    "\n",
    "# flags to pass to networks to set batch normalization layers\n",
    "# as trainable or not\n",
    "encoder_batch_trainable = tf.placeholder(tf.bool)\n",
    "decoder_batch_trainable = tf.placeholder(tf.bool)\n",
    "disc_batch_trainable = tf.placeholder(tf.bool)\n",
    "\n",
    "# encoder, decoder that will be connected to a discriminator\n",
    "# vae = Autoencoder(img_shape=(img_size, img_size, 3), zsize=zsize)\n",
    "# encoder = vae.encoder(X, encoder_batch_trainable)\n",
    "# decoder = vae.decoder(Z, decoder_batch_trainable)\n",
    "\n",
    "# a second decoder for decoding samplings of z\n",
    "# decoder_z_obj = Autoencoder(img_shape=(img_size, img_size, 3), zsize=zsize)\n",
    "# decoder_z = decoder_z_obj.decoder(Z, decoder_batch_trainable, reuse=True)\n",
    "\n",
    "# add noise to decoder generated images and inputs\n",
    "# decoder_plus_noise = decoder + tf.random_normal(\n",
    "#     shape=(batch_size, img_size, img_size, 3), mean=0.0, stddev=5.0)\n",
    "\n",
    "# X_plus_noise = X + tf.random_normal(\n",
    "#     shape=(batch_size, img_size, img_size, 3), mean=0.0, stddev=5.0)\n",
    "\n",
    "# discriminator attached to vae output\n",
    "disc_vae_obj = Discriminator(img_shape=(img_size, img_size, 3))\n",
    "disc_vae_obj.disc(R, disc_batch_trainable)\n",
    "disc_vae_logits = disc_vae_obj.logits\n",
    "\n",
    "# discriminator attached to X input\n",
    "# shares weights with other discriminator\n",
    "disc_x_obj = Discriminator(img_shape=(img_size, img_size, 3))\n",
    "disc_x_obj.disc(X, disc_batch_trainable, reuse=True)\n",
    "disc_x_logits = disc_x_obj.logits\n",
    "\n",
    "# discriminator attached to random Zs passed through decoder\n",
    "# shares weights with other discriminator\n",
    "# disc_z_obj = Discriminator(img_shape=(img_size, img_size, 3))\n",
    "# disc_z_obj.disc(decoder_z, disc_batch_trainable, reuse=True)\n",
    "# disc_z_logits = disc_z_obj.logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss functions and optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up loss functions and training_ops\n",
    "\n",
    "# latent loss used for training encoder\n",
    "# latent_loss = vae.latent_loss()\n",
    "\n",
    "# loss that uses decoder to determine similarity between\n",
    "# actual input images and output images from the vae\n",
    "# similarity_xentropy = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "#     labels=disc_x_obj.similarity, \n",
    "#     logits=disc_vae_obj.similarity)\n",
    "# similarity_loss = tf.reduce_mean(similarity_xentropy)\n",
    "\n",
    "# create noisy 0, 1 labels for discriminator loss\n",
    "# clip to [0, 1] because we are treating these as probabilities\n",
    "# _noisy0 = tf.zeros_like(disc_vae_logits) + tf.random_normal(\n",
    "#     shape=(batch_size, 1), mean=0.0, stddev=0.2)\n",
    "# noisy0 = tf.clip_by_value(_noisy0, 0.0, 1.0)\n",
    "\n",
    "# noisy1 = tf.ones_like(disc_x_logits) - 0.25\n",
    "\n",
    "# losses for the discriminator's output. Labels are real: 1, fake: 9.\n",
    "# cross entropy with 0 labels, since training prob that image is fake\n",
    "disc_vae_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "    labels=tf.zeros_like(disc_vae_logits),\n",
    "    logits=disc_vae_logits))\n",
    "\n",
    "# cross entropy with 1 labels, since training prob that image is fake\n",
    "disc_x_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "    labels=(tf.ones_like(disc_x_logits) - 0.1), # soft labeling trick\n",
    "    logits=disc_x_logits))\n",
    "\n",
    "# cross entropy with 1 labels, since training prob that image is fake\n",
    "# disc_z_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "#     labels=tf.ones_like(disc_z_logits),\n",
    "#     logits=disc_z_logits))\n",
    "\n",
    "# feed disc output back to decoder. Encoder is trying to spoof disc\n",
    "# so wants to push outputs of its fake images towards 1.\n",
    "# decoder_loss_from_disc = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "#     labels=tf.ones_like(disc_vae_logits),\n",
    "#     logits=disc_vae_logits))\n",
    "\n",
    "# minimize these with optimizer\n",
    "# disc_loss = disc_vae_loss + disc_x_loss + disc_z_loss\n",
    "disc_loss = disc_vae_loss + disc_x_loss\n",
    "# encoder_loss = latent_loss + similarity_loss\n",
    "\n",
    "# decoder_loss = loss_gamma * similarity_loss + decoder_loss_from_disc\n",
    "# decoder_loss = decoder_loss_from_disc\n",
    "\n",
    "# get weights to train for each of encoder, decoder, etc.\n",
    "# pass this to optimizer so it only trains w.r.t the network\n",
    "# we want to train and just uses other parts of the network as is\n",
    "# (for example use the discriminator to compute a loss during training\n",
    "# of the encoder, but don't adjust weights of the discriminator)\n",
    "\n",
    "# encoder_vars = [i for i in tf.trainable_variables() if 'encoder' in i.name]\n",
    "decoder_vars = [i for i in tf.trainable_variables() if 'decoder' in i.name]\n",
    "disc_vars = [i for i in tf.trainable_variables() if 'discriminator' in i.name]\n",
    "\n",
    "# encoder_update_ops = [i for i in tf.get_collection(tf.GraphKeys.UPDATE_OPS) if 'encoder' in i.name]\n",
    "decoder_update_ops = [i for i in tf.get_collection(tf.GraphKeys.UPDATE_OPS) if 'decoder' in i.name]\n",
    "disc_update_ops = [i for i in tf.get_collection(tf.GraphKeys.UPDATE_OPS) if 'discriminator' in i.name]\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta1)\n",
    "    \n",
    "# with tf.control_dependencies(encoder_update_ops):\n",
    "#     train_encoder = optimizer.minimize(encoder_loss, var_list=encoder_vars)\n",
    "    \n",
    "# with tf.control_dependencies(decoder_update_ops):\n",
    "#     train_decoder = optimizer.minimize(decoder_loss, var_list=decoder_vars)\n",
    "\n",
    "with tf.control_dependencies(disc_update_ops):\n",
    "    train_disc = optimizer.minimize(disc_loss, var_list=disc_vars)\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying to restore session\n",
      "INFO:tensorflow:Restoring parameters from /home/ec2-user/tf-checkpoints/vaegan-celeba/checkpoint.ckpt\n",
      "failed to restore session, creating a new one\n"
     ]
    }
   ],
   "source": [
    "# create or restore session\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "try:\n",
    "    print('trying to restore session')\n",
    "    saver.restore(sess, model_save_path)\n",
    "    print('restored session')\n",
    "except:\n",
    "    print('failed to restore session, creating a new one')\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "# write logs for tensorboard\n",
    "writer = tf.summary.FileWriter(log_directory, sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect data for tensorboard\n",
    "\n",
    "disc_vae_out = tf.reduce_mean(tf.sigmoid(disc_vae_logits))\n",
    "disc_x_out = tf.reduce_mean(tf.sigmoid(disc_x_logits))\n",
    "# disc_z_out = tf.reduce_mean(tf.sigmoid(disc_z_logits))\n",
    "\n",
    "# tf.summary.scalar('encoder_loss', encoder_loss)\n",
    "# tf.summary.scalar('decoder_loss', decoder_loss)\n",
    "tf.summary.scalar('discriminator_loss', disc_loss)\n",
    "# tf.summary.scalar('similarity_loss', similarity_loss)\n",
    "# tf.summary.scalar('decoder_loss_from_disc', decoder_loss_from_disc)\n",
    "# tf.summary.scalar('disc_vae_loss', disc_vae_loss)\n",
    "# tf.summary.scalar('disc_x_loss', disc_x_loss)\n",
    "# tf.summary.scalar('disc_z_loss', disc_z_loss)\n",
    "# tf.summary.scalar('latent_loss', latent_loss)\n",
    "\n",
    "tf.summary.scalar('disc_vae_out', disc_vae_out)\n",
    "tf.summary.scalar('disc_x_out', disc_x_out)\n",
    "# tf.summary.scalar('disc_z_out', disc_z_out)\n",
    "\n",
    "merged_summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_idx = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data to tensorboard log\n",
    "def report():\n",
    "    xfeed = training[:batch_size]\n",
    "    zfeed = np.random.normal(size=(batch_size, zsize))\n",
    "    rfeed = np.random.normal(size=(batch_size, img_size, img_size, 3), loc=0, scale=2.0)\n",
    "    summary = merged_summary.eval(feed_dict={\n",
    "        X: xfeed, \n",
    "        Z: zfeed,\n",
    "        R: rfeed,\n",
    "        encoder_batch_trainable: False,\n",
    "        decoder_batch_trainable: False,\n",
    "        disc_batch_trainable: False\n",
    "    })\n",
    "    writer.add_summary(summary, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training over 16 batches\n",
      "epoch 0 ................saving session\n",
      "epoch 1 ................saving session\n",
      "epoch 2 ................saving session\n",
      "epoch 3 ................saving session\n",
      "epoch 4 ................saving session\n",
      "epoch 5 ................saving session\n",
      "epoch 6 ................saving session\n",
      "epoch 7 ................saving session\n",
      "epoch 8 ................saving session\n",
      "epoch 9 ................saving session\n",
      "epoch 10 ................saving session\n",
      "epoch 11 ................saving session\n",
      "epoch 12 ................saving session\n",
      "epoch 13 ................saving session\n",
      "epoch 14 ................saving session\n",
      "epoch 15 ................saving session\n",
      "epoch 16 ................saving session\n",
      "epoch 17 ................saving session\n",
      "epoch 18 ................saving session\n",
      "epoch 19 ................saving session\n",
      "epoch 20 ................saving session\n",
      "epoch 21 ................saving session\n",
      "epoch 22 ................saving session\n",
      "epoch 23 ................saving session\n",
      "epoch 24 ................saving session\n",
      "epoch 25 ................saving session\n",
      "epoch 26 ................saving session\n",
      "epoch 27 ................saving session\n",
      "epoch 28 ................saving session\n",
      "epoch 29 ................saving session\n",
      "epoch 30 ................saving session\n",
      "epoch 31 ................saving session\n",
      "epoch 32 ................saving session\n",
      "epoch 33 ................saving session\n",
      "epoch 34 ................saving session\n",
      "epoch 35 ................saving session\n",
      "epoch 36 ................saving session\n",
      "epoch 37 ................saving session\n",
      "epoch 38 ................saving session\n",
      "epoch 39 ................saving session\n",
      "epoch 40 ................saving session\n",
      "epoch 41 ................saving session\n",
      "epoch 42 ................saving session\n",
      "epoch 43 ................saving session\n",
      "epoch 44 ................saving session\n",
      "epoch 45 ................saving session\n",
      "epoch 46 ................saving session\n",
      "epoch 47 ................saving session\n",
      "epoch 48 ................saving session\n",
      "epoch 49 ................saving session\n",
      "epoch 50 ................saving session\n",
      "epoch 51 ................saving session\n",
      "epoch 52 ................saving session\n",
      "epoch 53 ................saving session\n",
      "epoch 54 ................saving session\n",
      "epoch 55 ................saving session\n",
      "epoch 56 ................saving session\n",
      "epoch 57 ................saving session\n",
      "epoch 58 ................saving session\n",
      "epoch 59 ................saving session\n",
      "epoch 60 ................saving session\n",
      "epoch 61 ................saving session\n",
      "epoch 62 ................saving session\n",
      "epoch 63 ................saving session\n",
      "epoch 64 ................saving session\n",
      "epoch 65 ................saving session\n",
      "epoch 66 ................saving session\n",
      "epoch 67 ................saving session\n",
      "epoch 68 ................saving session\n",
      "epoch 69 ................saving session\n",
      "epoch 70 ................saving session\n",
      "epoch 71 ................saving session\n",
      "epoch 72 ................saving session\n",
      "epoch 73 ................saving session\n",
      "epoch 74 ................saving session\n",
      "epoch 75 ................saving session\n",
      "epoch 76 ................saving session\n",
      "epoch 77 ................saving session\n",
      "epoch 78 ................saving session\n",
      "epoch 79 ................saving session\n",
      "epoch 80 ................saving session\n",
      "epoch 81 ................saving session\n",
      "epoch 82 ................saving session\n",
      "epoch 83 ................saving session\n",
      "epoch 84 ................saving session\n",
      "epoch 85 ................saving session\n",
      "epoch 86 ................saving session\n",
      "epoch 87 ................saving session\n",
      "epoch 88 ................saving session\n",
      "epoch 89 ................saving session\n",
      "epoch 90 ................saving session\n",
      "epoch 91 ................saving session\n",
      "epoch 92 ................saving session\n",
      "epoch 93 ................saving session\n",
      "epoch 94 ................saving session\n",
      "epoch 95 ................saving session\n",
      "epoch 96 ................saving session\n",
      "epoch 97 ................saving session\n",
      "epoch 98 ................saving session\n",
      "epoch 99 ................saving session\n",
      "epoch 100 ................saving session\n",
      "epoch 101 ................saving session\n",
      "epoch 102 ................saving session\n",
      "epoch 103 ................saving session\n",
      "epoch 104 ................saving session\n",
      "epoch 105 ................saving session\n",
      "epoch 106 ................saving session\n",
      "epoch 107 ................saving session\n",
      "epoch 108 ................saving session\n",
      "epoch 109 ................saving session\n",
      "epoch 110 ................saving session\n",
      "epoch 111 ................saving session\n",
      "epoch 112 ................saving session\n",
      "epoch 113 ................saving session\n",
      "epoch 114 ................saving session\n",
      "epoch 115 ................saving session\n",
      "epoch 116 ................saving session\n",
      "epoch 117 ................saving session\n",
      "epoch 118 ................saving session\n",
      "epoch 119 ................saving session\n",
      "epoch 120 ................saving session\n",
      "epoch 121 ................saving session\n",
      "epoch 122 ................saving session\n",
      "epoch 123 ................saving session\n",
      "epoch 124 ................saving session\n",
      "epoch 125 ................saving session\n",
      "epoch 126 ................saving session\n",
      "epoch 127 ................saving session\n",
      "epoch 128 ................saving session\n",
      "epoch 129 ................saving session\n",
      "epoch 130 ................saving session\n",
      "epoch 131 ................saving session\n",
      "epoch 132 ................saving session\n",
      "epoch 133 ................saving session\n",
      "epoch 134 ................saving session\n",
      "epoch 135 ................saving session\n",
      "epoch 136 ................saving session\n",
      "epoch 137 ................saving session\n",
      "epoch 138 ................saving session\n",
      "epoch 139 ................saving session\n",
      "epoch 140 ................saving session\n",
      "epoch 141 ................saving session\n",
      "epoch 142 ................saving session\n",
      "epoch 143 ................saving session\n",
      "epoch 144 ................saving session\n",
      "epoch 145 ................saving session\n",
      "epoch 146 ................saving session\n",
      "epoch 147 ................saving session\n",
      "epoch 148 ................saving session\n",
      "epoch 149 ................saving session\n",
      "epoch 150 ................saving session\n",
      "epoch 151 ................saving session\n",
      "epoch 152 ................saving session\n",
      "epoch 153 ................saving session\n",
      "epoch 154 ................saving session\n",
      "epoch 155 ................saving session\n",
      "epoch 156 ................saving session\n",
      "epoch 157 ................saving session\n",
      "epoch 158 ................saving session\n",
      "epoch 159 ................saving session\n",
      "epoch 160 ................saving session\n",
      "epoch 161 ................saving session\n",
      "epoch 162 ................saving session\n",
      "epoch 163 ................saving session\n",
      "epoch 164 ................saving session\n",
      "epoch 165 ................saving session\n",
      "epoch 166 ................saving session\n",
      "epoch 167 ................saving session\n",
      "epoch 168 ................saving session\n",
      "epoch 169 ................saving session\n",
      "epoch 170 ................saving session\n",
      "epoch 171 ................saving session\n",
      "epoch 172 ................saving session\n",
      "epoch 173 ................saving session\n",
      "epoch 174 ................saving session\n",
      "epoch 175 ................saving session\n",
      "epoch 176 ................saving session\n",
      "epoch 177 ................saving session\n",
      "epoch 178 ................saving session\n",
      "epoch 179 ................saving session\n",
      "epoch 180 ................saving session\n",
      "epoch 181 ................saving session\n",
      "epoch 182 ................saving session\n",
      "epoch 183 ................saving session\n",
      "epoch 184 ................saving session\n",
      "epoch 185 ................saving session\n",
      "epoch 186 ................saving session\n",
      "epoch 187 ................saving session\n",
      "epoch 188 ................saving session\n",
      "epoch 189 ................saving session\n",
      "epoch 190 ................saving session\n",
      "epoch 191 ................saving session\n",
      "epoch 192 ................saving session\n",
      "epoch 193 ................saving session\n",
      "epoch 194 ................saving session\n",
      "epoch 195 ................saving session\n",
      "epoch 196 ................saving session\n",
      "epoch 197 ................saving session\n",
      "epoch 198 ................saving session\n",
      "epoch 199 ................saving session\n",
      "epoch 200 ................saving session\n",
      "epoch 201 ................saving session\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 202 ................saving session\n",
      "epoch 203 ................saving session\n",
      "epoch 204 ................saving session\n",
      "epoch 205 ................saving session\n",
      "epoch 206 ................saving session\n",
      "epoch 207 ................saving session\n",
      "epoch 208 ................saving session\n",
      "epoch 209 ................saving session\n",
      "epoch 210 ................saving session\n",
      "epoch 211 ................saving session\n",
      "epoch 212 ................saving session\n",
      "epoch 213 ................saving session\n",
      "epoch 214 ................saving session\n",
      "epoch 215 ................saving session\n",
      "epoch 216 ................saving session\n",
      "epoch 217 ................saving session\n",
      "epoch 218 ................saving session\n",
      "epoch 219 ................saving session\n",
      "epoch 220 ................saving session\n",
      "epoch 221 ................saving session\n",
      "epoch 222 ................saving session\n",
      "epoch 223 ................saving session\n",
      "epoch 224 ................saving session\n",
      "epoch 225 ................saving session\n",
      "epoch 226 ................saving session\n",
      "epoch 227 ................saving session\n",
      "epoch 228 ................saving session\n",
      "epoch 229 ................saving session\n",
      "epoch 230 ................saving session\n",
      "epoch 231 ................saving session\n",
      "epoch 232 ................saving session\n",
      "epoch 233 ................saving session\n",
      "epoch 234 ................saving session\n",
      "epoch 235 ................saving session\n",
      "epoch 236 ................saving session\n",
      "epoch 237 ................saving session\n",
      "epoch 238 ................saving session\n",
      "epoch 239 ................saving session\n",
      "epoch 240 ................saving session\n",
      "epoch 241 ................saving session\n",
      "epoch 242 ................saving session\n",
      "epoch 243 ................saving session\n",
      "epoch 244 ................saving session\n",
      "epoch 245 ................saving session\n",
      "epoch 246 ................saving session\n",
      "epoch 247 ................saving session\n",
      "epoch 248 ................saving session\n",
      "epoch 249 ................saving session\n",
      "epoch 250 ................saving session\n",
      "epoch 251 ................saving session\n",
      "epoch 252 ................saving session\n",
      "epoch 253 ................saving session\n",
      "epoch 254 ................saving session\n",
      "epoch 255 ................saving session\n",
      "epoch 256 ................saving session\n",
      "epoch 257 ................saving session\n",
      "epoch 258 ................saving session\n",
      "epoch 259 ................saving session\n",
      "epoch 260 ................saving session\n",
      "epoch 261 ................saving session\n",
      "epoch 262 ................saving session\n",
      "epoch 263 ................saving session\n",
      "epoch 264 ................saving session\n",
      "epoch 265 ................saving session\n",
      "epoch 266 ................saving session\n",
      "epoch 267 ................saving session\n",
      "epoch 268 ................saving session\n",
      "epoch 269 ................saving session\n",
      "epoch 270 ................saving session\n",
      "epoch 271 ................saving session\n",
      "epoch 272 ................saving session\n",
      "epoch 273 ................saving session\n",
      "epoch 274 ................saving session\n",
      "epoch 275 ................saving session\n",
      "epoch 276 ................saving session\n",
      "epoch 277 ................saving session\n",
      "epoch 278 ................saving session\n",
      "epoch 279 ................saving session\n",
      "epoch 280 ................saving session\n",
      "epoch 281 ................saving session\n",
      "epoch 282 ................saving session\n",
      "epoch 283 ................saving session\n",
      "epoch 284 ................saving session\n",
      "epoch 285 ................saving session\n",
      "epoch 286 ................saving session\n",
      "epoch 287 ................saving session\n",
      "epoch 288 ................saving session\n",
      "epoch 289 ................saving session\n",
      "epoch 290 ................saving session\n",
      "epoch 291 ................saving session\n",
      "epoch 292 ................saving session\n",
      "epoch 293 ................saving session\n",
      "epoch 294 ................saving session\n",
      "epoch 295 ................saving session\n",
      "epoch 296 ................saving session\n",
      "epoch 297 ................saving session\n",
      "epoch 298 ................saving session\n",
      "epoch 299 ................saving session\n",
      "epoch 300 ................saving session\n",
      "epoch 301 ................saving session\n",
      "epoch 302 ................saving session\n",
      "epoch 303 ................saving session\n",
      "epoch 304 ................saving session\n",
      "epoch 305 ................saving session\n",
      "epoch 306 ................saving session\n",
      "epoch 307 ................saving session\n",
      "epoch 308 ................saving session\n",
      "epoch 309 ................saving session\n",
      "epoch 310 ................saving session\n",
      "epoch 311 ................saving session\n",
      "epoch 312 ................saving session\n",
      "epoch 313 ................saving session\n",
      "epoch 314 ................saving session\n",
      "epoch 315 ................saving session\n",
      "epoch 316 ................saving session\n",
      "epoch 317 ................saving session\n",
      "epoch 318 ................saving session\n",
      "epoch 319 ................saving session\n",
      "epoch 320 ."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-cef3741b9d83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mencoder_batch_trainable\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mdecoder_batch_trainable\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                 \u001b[0mdisc_batch_trainable\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             })\n\u001b[1;32m     32\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/keras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/keras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/keras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/keras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/keras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import math\n",
    "epochs = 10000\n",
    "batches = int(float(training_set_size) / batch_size)\n",
    "print('training over %s batches' % batches)\n",
    "# number of iterations to train per epoch, so I can easily\n",
    "# train, e.g. 2x disc for each encoder, decoder; or skip trianing\n",
    "do_train = {\n",
    "    'encoder': 0,\n",
    "    'decoder': 0,\n",
    "    'disc':    1\n",
    "}\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print ('epoch %s ' % epoch, end='')\n",
    "    zdraws = np.random.normal(size=(training_set_size, zsize))\n",
    "    rdraws = np.random.normal(size=(training_set_size, img_size, img_size, 3), loc=0, scale=2.0)\n",
    "    \n",
    "    # train discriminator\n",
    "    for _ in range(do_train['disc']):\n",
    "        for batch in range(batches):\n",
    "            xfeed = training[batch*batch_size:(batch+1)*batch_size]\n",
    "            zfeed = zdraws[batch*batch_size:(batch+1)*batch_size]\n",
    "            rfeed = rdraws[batch*batch_size:(batch+1)*batch_size]\n",
    "            sess.run(train_disc, feed_dict={\n",
    "                X: xfeed,\n",
    "                Z: zfeed,\n",
    "                R: rfeed,\n",
    "                encoder_batch_trainable: False,\n",
    "                decoder_batch_trainable: False,\n",
    "                disc_batch_trainable: True\n",
    "            })\n",
    "            print('.', end='')\n",
    "    report()\n",
    "    \n",
    "#     # train encoder\n",
    "#     for _ in range(do_train['encoder']):\n",
    "#         for batch in range(batches):\n",
    "#             xfeed = training[batch*batch_size:(batch+1)*batch_size]\n",
    "#             sess.run(train_encoder, feed_dict={\n",
    "#                 X: xfeed,\n",
    "#                 encoder_batch_trainable: True,\n",
    "#                 decoder_batch_trainable: False,\n",
    "#                 disc_batch_trainable: False\n",
    "#                 })\n",
    "#             print('.', end='')\n",
    "        \n",
    "#     # train decoder\n",
    "#     for _ in range(do_train['decoder']):\n",
    "#         for batch in range(batches):\n",
    "#             xfeed = training[batch*batch_size:(batch+1)*batch_size]\n",
    "#             zfeed = zdraws[batch*batch_size:(batch+1)*batch_size]\n",
    "#             sess.run(train_decoder, feed_dict={\n",
    "#                 X: xfeed, \n",
    "#                 Z: zfeed,\n",
    "#                 encoder_batch_trainable: False,\n",
    "#                 decoder_batch_trainable: True,\n",
    "#                 disc_batch_trainable: False\n",
    "#             })\n",
    "#             print('.', end='')\n",
    "#     report()\n",
    "#     print('')\n",
    "    \n",
    "    if (epoch % 1 == 0):\n",
    "        print('saving session', flush=True)\n",
    "        saver.save(sess, model_save_path)\n",
    "                        \n",
    "#         example = decoder.eval(feed_dict={\n",
    "#             X: training[:1],\n",
    "#             Z: zdraws[:1],\n",
    "#             encoder_batch_trainable: False,\n",
    "#             decoder_batch_trainable: False,\n",
    "#             disc_batch_trainable: False\n",
    "#         })\n",
    "#         img_save_path = os.path.join(outputs_directory, '%06d.jpg' % img_idx)\n",
    "#         img_idx += 1\n",
    "#         sp.misc.imsave(img_save_path, pixels01(example[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zdraws = np.random.normal(size=(64, zsize))\n",
    "generated = decoder.eval(feed_dict={\n",
    "    Z: zdraws,\n",
    "    encoder_batch_trainable: False,\n",
    "    decoder_batch_trainable: False,\n",
    "    disc_batch_trainable: False\n",
    "})\n",
    "\n",
    "generated01 = pixels01(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imshow(generated01[0:8])\n",
    "_disc_x = disc_x_obj.disc(X, disc_batch_trainable, reuse=True)\n",
    "outs = _disc_x.eval(feed_dict={\n",
    "    X: training[0:64],\n",
    "    encoder_batch_trainable: False,\n",
    "    decoder_batch_trainable: False,\n",
    "    disc_batch_trainable: False\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs[0:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
