{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "\n",
    "# img_directory = '/Users/rwilliams/Desktop/celeba/training'\n",
    "img_directory = '/home/ec2-user/training-data/img_align_celeba'\n",
    "model_save_path = '/home/ec2-user/tf-checkpoints/vaegan-celeba/checkpoint.ckpt'\n",
    "outputs_directory = '/home/ec2-user/outputs/vaegan-celeba'\n",
    "log_directory = '/home/ec2-user/tf-logs/vaegan-celeba'\n",
    "batch_size = 64\n",
    "training_set_size = 5000\n",
    "img_size = 64\n",
    "\n",
    "# for adam optimizer\n",
    "learning_rate = 2e-4\n",
    "learning_beta1 = 0.5\n",
    "# learning_beta1 = 0.9\n",
    "\n",
    "zsize = 128\n",
    "\n",
    "# weights similarity loss term for decoder loss\n",
    "# loss_gamma = 1e-2\n",
    "# trying higher gamma\n",
    "loss_gamma = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Jupyter imports\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import os\n",
    "from utils import imshow, resize_crop, load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.conda/envs/keras/lib/python3.6/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    }
   ],
   "source": [
    "# load training data\n",
    "training = np.array([resize_crop(load_img(i+1, img_directory), (img_size, img_size)) for i in range(training_set_size)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create models\n",
    "\n",
    "import tensorflow as tf\n",
    "from autoencoder import Autoencoder\n",
    "from discriminator import Discriminator\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(42.0)\n",
    "\n",
    "# input images feed\n",
    "X = tf.placeholder(tf.float32, [None, img_size, img_size, 3])\n",
    "\n",
    "# for feeding random draws of z (latent variable)\n",
    "Z = tf.placeholder(tf.float32, [None, zsize])\n",
    "\n",
    "# encoder, decoder that will be connected to a discriminator\n",
    "vae = Autoencoder(img_shape=(img_size, img_size, 3), zsize=zsize)\n",
    "encoder = vae.encoder(X)\n",
    "decoder = vae.decoder(encoder)\n",
    "\n",
    "# a second decoder for decoding samplings of z\n",
    "decoder_z_obj = Autoencoder(img_shape=(img_size, img_size, 3), zsize=zsize)\n",
    "decoder_z = decoder_z_obj.decoder(Z, reuse=True)\n",
    "\n",
    "# discriminator attached to vae output\n",
    "disc_vae_obj = Discriminator(img_shape=(img_size, img_size, 3))\n",
    "disc_vae_obj.disc(decoder)\n",
    "disc_vae_logits = disc_vae_obj.logits\n",
    "\n",
    "# discriminator attached to X input\n",
    "# shares weights with other discriminator\n",
    "disc_x_obj = Discriminator(img_shape=(img_size, img_size, 3))\n",
    "disc_x_obj.disc(X, reuse=True)\n",
    "disc_x_logits = disc_x_obj.logits\n",
    "\n",
    "# discriminator attached to random Zs passed through decoder\n",
    "# shares weights with other discriminator\n",
    "disc_z_obj = Discriminator(img_shape=(img_size, img_size, 3))\n",
    "disc_z_obj.disc(decoder_z, reuse=True)\n",
    "disc_z_logits = disc_z_obj.logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss functions and optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up loss functions and training_ops\n",
    "\n",
    "# latent loss used for training encoder\n",
    "latent_loss = vae.latent_loss()\n",
    "\n",
    "# loss that uses decoder to determine similarity between\n",
    "# actual input images and output images from the vae\n",
    "similarity_xentropy = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "    labels=disc_x_obj.similarity, \n",
    "    logits=disc_vae_obj.similarity)\n",
    "similarity_loss = tf.reduce_mean(similarity_xentropy)\n",
    "\n",
    "# losses for the discriminator's output. Labels are real: 0, fake: 1.\n",
    "# cross entropy with 1 labels, since training prob that image is fake\n",
    "disc_vae_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "    labels=tf.ones_like(disc_vae_logits),\n",
    "    logits=disc_vae_logits))\n",
    "\n",
    "# cross entropy with 0 labels, since training prob that image is fake\n",
    "disc_x_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "    labels=tf.zeros_like(disc_x_logits),\n",
    "    logits=disc_x_logits))\n",
    "\n",
    "# cross entropy with 1 labels, since training prob that image is fake\n",
    "disc_z_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "    labels=tf.ones_like(disc_z_logits),\n",
    "    logits=disc_z_logits))\n",
    "\n",
    "# minimize these with optimizer\n",
    "disc_loss = disc_vae_loss + disc_x_loss + disc_z_loss\n",
    "encoder_loss = latent_loss + similarity_loss\n",
    "decoder_loss = loss_gamma * similarity_loss - disc_loss\n",
    "\n",
    "# get weights to train for each of encoder, decoder, etc.\n",
    "# pass this to optimizer so it only trains w.r.t the network\n",
    "# we want to train and just uses other parts of the network as is\n",
    "# (for example use the discriminator to compute a loss during training\n",
    "# of the encoder, but don't adjust weights of the discriminator)\n",
    "\n",
    "encoder_vars = [i for i in tf.trainable_variables() if 'encoder' in i.name]\n",
    "decoder_vars = [i for i in tf.trainable_variables() if 'decoder' in i.name]\n",
    "disc_vars = [i for i in tf.trainable_variables() if 'discriminator' in i.name]\n",
    "\n",
    "encoder_update_ops = [i for i in tf.get_collection(tf.GraphKeys.UPDATE_OPS) if 'encoder' in i.name]\n",
    "decoder_update_ops = [i for i in tf.get_collection(tf.GraphKeys.UPDATE_OPS) if 'decoder' in i.name]\n",
    "disc_update_ops = [i for i in tf.get_collection(tf.GraphKeys.UPDATE_OPS) if 'discriminator' in i.name]\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=learning_beta1)\n",
    "    \n",
    "with tf.control_dependencies(encoder_update_ops):\n",
    "    train_encoder = optimizer.minimize(encoder_loss, var_list=encoder_vars)\n",
    "    \n",
    "with tf.control_dependencies(decoder_update_ops):\n",
    "    train_decoder = optimizer.minimize(decoder_loss, var_list=decoder_vars)\n",
    "\n",
    "with tf.control_dependencies(disc_update_ops):\n",
    "    train_disc = optimizer.minimize(disc_loss, var_list=disc_vars)\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying to restore session\n",
      "INFO:tensorflow:Restoring parameters from /home/ec2-user/tf-checkpoints/vaegan-celeba/checkpoint.ckpt\n",
      "failed to restore session, creating a new one\n"
     ]
    }
   ],
   "source": [
    "# create or restore session\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "try:\n",
    "    print('trying to restore session')\n",
    "    saver.restore(sess, model_save_path)\n",
    "    print('restored session')\n",
    "except:\n",
    "    print('failed to restore session, creating a new one')\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "# write logs for tensorboard\n",
    "writer = tf.summary.FileWriter(log_directory, sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# collect data for tensorboard\n",
    "\n",
    "disc_vae_out = tf.reduce_mean(tf.sigmoid(disc_vae_logits))\n",
    "disc_x_out = tf.reduce_mean(tf.sigmoid(disc_x_logits))\n",
    "disc_z_out = tf.reduce_mean(tf.sigmoid(disc_z_logits))\n",
    "\n",
    "tf.summary.scalar('encoder_loss', encoder_loss)\n",
    "tf.summary.scalar('decoder_loss', decoder_loss)\n",
    "tf.summary.scalar('discriminator_loss', disc_loss)\n",
    "tf.summary.scalar('similarity_loss', similarity_loss)\n",
    "tf.summary.scalar('disc_vae_loss', disc_vae_loss)\n",
    "tf.summary.scalar('disc_x_loss', disc_x_loss)\n",
    "tf.summary.scalar('disc_z_loss', disc_z_loss)\n",
    "tf.summary.scalar('latent_loss', latent_loss)\n",
    "\n",
    "tf.summary.scalar('disc_vae_out', disc_vae_out)\n",
    "tf.summary.scalar('disc_x_out', disc_x_out)\n",
    "tf.summary.scalar('disc_z_out', disc_z_out)\n",
    "\n",
    "merged_summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_idx = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 ..........................................................................................................................................................................................................................................\n",
      "saving session\n",
      "epoch 1 ..........................................................................................................................................................................................................................................\n",
      "saving session\n",
      "epoch 2 ..........................................................................................................................................................................................................................................\n",
      "saving session\n",
      "epoch 3 ..........................................................................................................................................................................................................................................\n",
      "saving session\n",
      "epoch 4 ..........................................................................................................................................................................................................................................\n",
      "saving session\n",
      "epoch 5 ..........................................................................................................................................................................................................................................\n",
      "saving session\n",
      "epoch 6 ..........................................................................................................................................................................................................................................\n",
      "saving session\n",
      "epoch 7 ..........................................................................................................................................................................................................................................\n",
      "saving session\n",
      "epoch 8 ..........................................................................................................................................................................................................................................\n",
      "saving session\n",
      "epoch 9 ..........................................................................................................................................................................................................................................\n",
      "saving session\n",
      "epoch 10 ..........................................................................................................................................................................................................................................\n",
      "saving session\n",
      "epoch 11 ..........................................................................................................................................................................................................................................\n",
      "saving session\n",
      "epoch 12 ..........................................................................................................................................................................................................................................\n",
      "saving session\n",
      "epoch 13 ..........................................................................................................................................................................................................................................\n",
      "saving session\n",
      "epoch 14 ..........................................................................................................................................................................................................................................\n",
      "saving session\n",
      "epoch 15 ..........................................................................................................................................................................................................................................\n",
      "saving session\n",
      "epoch 16 ..........................................................................................................................................................................................................................................\n",
      "saving session\n",
      "epoch 17 ..........................................................................................................................................................................................................................................\n",
      "saving session\n",
      "epoch 18 ..........................................................................................................................................................................................................................................\n",
      "saving session\n",
      "epoch 19 ..........................................................................................................................................................................................................................................\n",
      "saving session\n",
      "epoch 20 ..........................................................................................................................................................................................................................................\n",
      "saving session\n",
      "epoch 21 ..........................................................................................................................................................................................................................................\n",
      "saving session\n",
      "epoch 22 ..........................................................................................................................................................................................................................................\n",
      "saving session\n",
      "epoch 23 ..........................................................................................................................................................................................................................................\n",
      "saving session\n",
      "epoch 24 ..........................................................................................................................................................................................................................................\n",
      "saving session\n",
      "epoch 25 ..........................................................................................................................................................................................................................................\n",
      "saving session\n",
      "epoch 26 ..........................................................................................................................................................................................................................................\n",
      "saving session\n",
      "epoch 27 ..........................................................................................................................................................................................................................................\n",
      "saving session\n",
      "epoch 28 ..........................................................................................................................................................................................................................................\n",
      "saving session\n",
      "epoch 29 ..........................................................................................................................................................................................................................................\n",
      "saving session\n",
      "epoch 30 ..........................................................................................................................................................................................................................................\n",
      "saving session\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 31 ..........................................................................................................................................................................................................................................\n",
      "saving session\n",
      "epoch 32 ..........................................................................................................................................................................................................................................\n",
      "saving session\n",
      "epoch 33 ..........................................................................................................................................................................................................................................\n",
      "saving session\n",
      "epoch 34 ..........................................................................................................................................................................................................................................\n",
      "saving session\n",
      "epoch 35 ..........................................................................................................................................................................................................................................\n",
      "saving session\n",
      "epoch 36 ..........................................................................................................................................................................................................................................\n",
      "saving session\n",
      "epoch 37 ..........................................................................................................................................................................................................................................\n",
      "saving session\n",
      "epoch 38 ..........................................................................................................................................................................................................................................\n",
      "saving session\n",
      "epoch 39 ..........................................................................................................................................................................................................................................\n",
      "saving session\n",
      "epoch 40 ..........................................................................................................................................................................................................................................\n",
      "saving session\n",
      "epoch 41 ..........................................................................................................................................................................................................................................\n",
      "saving session\n",
      "epoch 42 ..........."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-821d35bd9489>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mxfeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mzfeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzdraws\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_disc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mxfeed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mzfeed\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/keras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/keras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/keras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/keras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/keras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import math\n",
    "epochs = 10000\n",
    "batches = int(float(training_set_size) / batch_size)\n",
    "train_discriminator = True\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print ('epoch %s ' % epoch, end='')\n",
    "    zdraws = np.random.normal(size=(training_set_size, zsize))\n",
    "    \n",
    "    # train discriminator\n",
    "    if (train_discriminator):\n",
    "        for batch in range(batches):\n",
    "            xfeed = training[batch*batch_size:(batch+1)*batch_size]\n",
    "            zfeed = zdraws[batch*batch_size:(batch+1)*batch_size]\n",
    "            sess.run(train_disc, feed_dict={X: xfeed, Z: zfeed})\n",
    "            print('.', end='')\n",
    "         \n",
    "    # train encoder\n",
    "    for batch in range(batches):\n",
    "        xfeed = training[batch*batch_size:(batch+1)*batch_size]\n",
    "        sess.run(train_encoder, feed_dict={X: xfeed})\n",
    "        print('.', end='')\n",
    "        \n",
    "    # train decoder\n",
    "    for batch in range(batches):\n",
    "        xfeed = training[batch*batch_size:(batch+1)*batch_size]\n",
    "        zfeed = zdraws[batch*batch_size:(batch+1)*batch_size]\n",
    "        sess.run(train_decoder, feed_dict={X: xfeed, Z: zfeed})\n",
    "        print('.', end='')\n",
    "        \n",
    "    print('')\n",
    "    \n",
    "    if (epoch % 1 == 0):\n",
    "        print('saving session', flush=True)\n",
    "        saver.save(sess, model_save_path)\n",
    "        \n",
    "        xfeed = training[:batch_size]\n",
    "        zfeed = zdraws[:batch_size]\n",
    "        summary = merged_summary.eval(feed_dict={X: xfeed, Z: zfeed})\n",
    "        writer.add_summary(summary, epoch)\n",
    "        \n",
    "#         disc_vae_out_val = disc_vae_out.eval(feed_dict={X: xfeed})\n",
    "#         if (disc_vae_out_val >= 0.5):\n",
    "#             train_discriminator = False\n",
    "#             print('stopping training of discriminator')\n",
    "#         else:\n",
    "#             train_discriminator = True\n",
    "#             print('starting training of discriminator')\n",
    "            \n",
    "        example = decoder.eval(feed_dict={X: training[:1]})\n",
    "        img_save_path = os.path.join(outputs_directory, '%06d.jpg' % img_idx)\n",
    "        img_idx += 1\n",
    "        sp.misc.imsave(img_save_path, example[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# vae_out = decoder.eval(feed_dict={X: training[:4]})\n",
    "# imshow(training[:4])\n",
    "# imshow(vae_out[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# r = np.random.normal(size=(8,128), scale=1.0)\n",
    "# y = sess.run(decoder, feed_dict={encoder: r})\n",
    "# imshow(y[0:8])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
